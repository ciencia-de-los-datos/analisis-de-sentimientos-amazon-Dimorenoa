{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd44d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Análisis de Sentimientos usando Naive Bayes\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "El archivo `amazon_cells_labelled.txt` contiene una serie de comentarios sobre productos\n",
    "de la tienda de amazon, los cuales están etiquetados como positivos (=1) o negativos (=0)\n",
    "o indterminados (=NULL). En este taller se construirá un modelo de clasificación usando\n",
    "Naive Bayes para determinar el sentimiento de un comentario.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def pregunta_01():\n",
    "    \"\"\"\n",
    "    Carga de datos.\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Lea el archivo `amazon_cells_labelled.tsv` y cree un DataFrame usando pandas.\n",
    "    # Etiquete la primera columna como `msg` y la segunda como `lbl`. Esta función\n",
    "    # retorna el dataframe con las dos columnas.\n",
    "    df = pd.read_csv(\n",
    "         \"amazon_cells_labelled.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=[\"msg\", \"lbl\"],\n",
    "    )\n",
    "    # Separe los grupos de mensajes etiquetados y no etiquetados.\n",
    "    df_tagged = df[df[\"lbl\"].notnull()]\n",
    "    df_untagged = df[df[\"lbl\"].isnull()\n",
    "    ]\n",
    "    x_tagged = df_tagged[\"msg\"]\n",
    "    y_tagged = df_tagged[\"lbl\"]\n",
    "\n",
    "    x_untagged = df_untagged[\"msg\"]\n",
    "    y_untagged = df_untagged[\"lbl\"]\n",
    "\n",
    "    # Retorne los grupos de mensajes\n",
    "    return x_tagged, y_tagged, x_untagged, y_untagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e6e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_02():\n",
    "    \"\"\"\n",
    "    Preparación de los conjuntos de datos.\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe train_test_split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Cargue los datos generados en la pregunta 01.\n",
    "    x_tagged, y_tagged, x_untagged, y_untagged = pregunta_01()\n",
    "\n",
    "    # Divida los datos de entrenamiento y prueba. La semilla del generador de números\n",
    "    # aleatorios es 12345. Use el 10% de patrones para la muestra de prueba.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_tagged,\n",
    "        y_tagged,\n",
    "        test_size=0.10,\n",
    "        random_state=12345,\n",
    "    )\n",
    "\n",
    "    # Retorne `X_train`, `X_test`, `y_train` y `y_test`\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_03():\n",
    "    \"\"\"\n",
    "    Construcción de un analizador de palabras\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    # Importe el stemmer de Porter\n",
    "    # Importe CountVectorizer\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # Cree un stemeer que use el algoritmo de Porter.\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Cree una instancia del analizador de palabras (build_analyzer)\n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "    # Retorne el analizador de palabras\n",
    "    return lambda x: (stemmer.stem(w) for w in analyzer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeecc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_04():\n",
    "    \"\"\"\n",
    "    Especificación del pipeline y entrenamiento\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe CountVetorizer\n",
    "    # Importe GridSearchCV\n",
    "    # Importe Pipeline\n",
    "    # Importe BernoulliNB\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "    # Cargue las variables.\n",
    "    x_train, x_test, y_train, y_test= pregunta_02()\n",
    "\n",
    "    # Obtenga el analizador de la pregunta 3.\n",
    "    analyzer = pregunta_03()\n",
    "\n",
    "    # Cree una instancia de CountVectorizer que use el analizador de palabras\n",
    "    # de la pregunta 3. Esta instancia debe retornar una matriz binaria. El\n",
    "    # límite superior para la frecuencia de palabras es del 100% y un límite\n",
    "    # inferior de 5 palabras. Solo deben analizarse palabras conformadas por\n",
    "    # letras.\n",
    "    countVectorizer = CountVectorizer(\n",
    "        analyzer=analyzer,\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "        token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z]+\\b\",\n",
    "        binary=True,\n",
    "        max_df=1.0,\n",
    "        min_df=5,\n",
    "    )\n",
    "    \n",
    "    # Cree un pipeline que contenga el CountVectorizer y el modelo de BernoulliNB.\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"cv\", countVectorizer),\n",
    "            (\"clf\", BernoulliNB()),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Defina un diccionario de parámetros para el GridSearchCV. Se deben\n",
    "    # considerar 10 valores entre 0.1 y 1.0 para el parámetro alpha de\n",
    "    # BernoulliNB.\n",
    "    param_grid = {\n",
    "        \"clf__alpha\": np.linspace(0.1,1.0,10),\n",
    "    }\n",
    "\n",
    "    # Defina una instancia de GridSearchCV con el pipeline y el diccionario de\n",
    "    # parámetros. Use cv = 5, y \"accuracy\" como métrica de evaluación\n",
    "    gridSearchCV = GridSearchCV(\n",
    "        estimator= pipeline,\n",
    "        param_grid= param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        refit=True,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Búsque la mejor combinación de regresores\n",
    "    gridSearchCV.fit(x_train, y_train)\n",
    "\n",
    "    # Retorne el mejor modelo\n",
    "    return gridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64599598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_05():\n",
    "    \"\"\"\n",
    "    Evaluación del modelo\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Importe confusion_matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # Obtenga el pipeline de la pregunta 3.\n",
    "    gridSearchCV = pregunta_04()\n",
    "\n",
    "    # Cargue las variables.\n",
    "    X_train, X_test, y_train, y_test = pregunta_02()\n",
    "\n",
    "    # Evalúe el pipeline con los datos de entrenamiento usando la matriz de confusion.\n",
    "    cfm_train = confusion_matrix(\n",
    "        y_true= y_train,\n",
    "        y_pred = gridSearchCV.predict(X_train),\n",
    "    )\n",
    "\n",
    "    cfm_test = confusion_matrix(\n",
    "        y_true= y_test,\n",
    "        y_pred= gridSearchCV.predict(X_test),\n",
    "    )\n",
    "\n",
    "    # Retorne la matriz de confusion de entrenamiento y prueba\n",
    "    return cfm_train, cfm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a42f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregunta_06():\n",
    "    \"\"\"\n",
    "    Pronóstico\n",
    "    -------------------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenga el pipeline de la pregunta 3.\n",
    "    gridSearchCV = pregunta_04()\n",
    "\n",
    "    # Cargue los datos generados en la pregunta 01.\n",
    "    _, _, X_untagged, _ = pregunta_01()\n",
    "\n",
    "    # pronostique la polaridad del sentimiento para los datos\n",
    "    # no etiquetados\n",
    "    y_untagged_pred = gridSearchCV.predict(X_untagged)\n",
    "\n",
    "    # Retorne el vector de predicciones\n",
    "    return y_untagged_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377eaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
